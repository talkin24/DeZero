# TIL - Deep Learning from Scratch 3

### Step1.

- `self.변수`는 인스턴스 변수

- 벡터를 다룰때 차원이라는 말에 주의. 3차원 벡터와 3차원 배열은 다른 뜻.

  

### Step2.

- `__call__` 메서드는 파이썬의 특수 메서드. 이 메서드를 정의하면 f = Function() 형태로 함수의 인스턴스를 변수 f에 대입해두고, 나중에 f(...) 형태로 `__call__` 메서드를 호출할 수 있음
- 상속해서 사용해야할 메서드를 직접 호출하지 않게 하기위해 예외를 발생시킬 수 있음.



### Step3.

- 계산그래프를 이용하면 각 변수에 대한 미분을 효율적으로 계산할 수 있음



### Step4.

- 수치미분: 극한을 이용할 수 없으니, 아주 미세한 차이를 이용하여 함수의 변화량을 구하는 방법
- 중앙차분: 수치미분에서 발생할 수밖에 없는 근사오차를 줄이기 위해 사용. f(x)와 f(x+h)의 차이를 구하는 대신, f(x-h)와 f(x+h)의 차이를 구함
  - 중앙차분에서 직선의 기울기는 f(x + h) - f(x - h) / 2h
- 해석적으로 계산한다고 함은 수식 변형만으로 답을 유도한다는 뜻
- 수치미분의 큰 문제1: 오차가 존재한다는 것. 수치미분에 오차가 포함되기 쉬운 이유는 '자릿수 누락'
- 수미치분의 큰 문제2: 계산량이 많음
- 위의 2가지 문제를 해결하기 위해 등장한 것이 backpropagation.
- 역전파는 복잡한 알고리즘이라 버그가 섞여들어가기가 쉬움. 그래서 역전파를 정확히 구현했는지 확인하기 위해 수치 미분의 결과를 이용(기울기 확인)



### Step5.

- 수치미분은 계산 비용과 정확도 면에서 문제가 있음 -> 역전파로 해결
- 역전파를 이용하면 미분을 효율적으로 계산할 수 있고 결괏값의 오차도 더 작음
- 합성함수의 미분은 구성 함수들이 미분의 곱으로 분해할 수 있음
- 출력에서 입력 방향으로 계산되는 이유는 y의 미분값을 전파하기 위해서, 즉 y를 중요요소로 대우하기 때문
- 머신러닝은 주로 대량의 매개변수를 입력받아 마지막 손실함수를 거쳐 출력을 내보냄
  - 이때 손실함수의 출력은 단일한 스칼라값이며, 이 값이 중요요소임
  - 따라서 손실함수의 각 매개변수에 대한 미분 계산이 필요한데, 이 경우 미분값을 출력에서 입력 방향으로 전파하면 한 번의 전파만으로 모든 매개변수에 대한 미분을 계산할 수 있음
  - 이렇게 계산이 효율적으로 이뤄지므로 미분을 반대방향으로 전파하는 방식을 이용함
- 순전파의 경우, 통상값. 역전파는 미분값 계산에 유리
- 역전파 시에는 순전파 시 이용한 데이터가 필요!
  - 따라서 역전파를 구현하기 위해선 순전파를 먼저 수행하고, 이때 각 함수가 입력 변수의 값을 기억해 두어야 함



### Step6.

- 순전파에서 input을 self.input에 저장하여 역전파 시 사용



### Step 7.

- Define-by-Run: 딥러닝에서 수행하는 계산들을 계산 시점에 '연결'하는 방식. '동적 계산 그래프'
-  리스트 데이터 구조를 응용하면 수행한 계산을 리스트에 추가해 나가는 것만으로 어떠한 계산 그래프의 역전파도 제대로 해낼 수 있음(웬거트 리스트)
- 변수 관점에서 함수는 창조자 혹은 부모
- 동적 계산 그래프는 실제로 계산이 이루어질 때 변수에 관련 '연결'을 기록하는 방식으로 만들어짐
- 연결된 Variable과 Function이 있다면 계산 그래프를 거꾸로 거슬러 올라갈 수 있음
- assert문은 그 평가 결과가 True가 아니면 예외가 발생. 따라서 조건을 충족하는지 여부를 확인하는데 사용할 수 있음
- 역전파 구현 순서
  1. 함수를 가져온다
  2. 함수의 입력을 가져온다
  3. 함수의 backward 메서드를 호출한다



### Step 8.

- 재귀를 반복으로 바꿈으로써 효율 개선 가능
- 재귀는 함수를 재귀적으로 호출할 때마다 중간 결과를 메모리에 유지하면서 처리를 이어감
- 따라서 반복문의 효율이 일반적으로 더 좋음. 그러나 메모리 성능의 확장으로 보통 재귀 사용은 크게 문제가 되진 않음

